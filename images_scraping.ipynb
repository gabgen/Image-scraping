{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.5"
    },
    "colab": {
      "name": "scraping.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r1SNzh44uOk"
      },
      "source": [
        "from google.colab import drive\r\n",
        "\r\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcTcO7bK5loN"
      },
      "source": [
        "#!pip install selenium tqdm requests pillow\r\n",
        "#!apt-get update \r\n",
        "#!apt install chromium-chromedriver\r\n",
        "#!pip install webp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gtnQa1_04mei"
      },
      "source": [
        "from selenium import webdriver\n",
        "import urllib\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import io\n",
        "import requests\n",
        "from PIL import Image\n",
        "\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UjALSDwyaCt"
      },
      "source": [
        "def list_folds_to_complete(food_path):\r\n",
        "\r\n",
        "  food_list=os.listdir(food_path)\r\n",
        "\r\n",
        "  empty_folders=[]\r\n",
        "  for j in range(len(food_list)):\r\n",
        "    if not os.listdir(food_path+\"/\"+food_list[j]):\r\n",
        "      empty_folders.append(food_list[j])\r\n",
        "\r\n",
        "  for i in range(len(food_list)):\r\n",
        "    food_list[i]=food_list[i].replace(\"_\",\" \")\r\n",
        "\r\n",
        "  for i in range(len(empty_folders)):\r\n",
        "    empty_folders[i]=empty_folders[i].replace(\"_\",\" \")\r\n",
        "\r\n",
        "  return empty_folders\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def rmv_emp_folds(food_path,emp_folds):\r\n",
        "\r\n",
        "  for folder in emp_folds:\r\n",
        "    folder=folder.replace(\" \",\"_\")\r\n",
        "    os.rmdir(food_path+\"/\"+folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dmx3PJcS4meq"
      },
      "source": [
        "def fetch_image_urls(query:str, max_links_to_fetch:int, wd:webdriver, sleep_between_interactions:int):\n",
        "    \n",
        "    \n",
        "    def scroll_to_end(wd):\n",
        "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
        "        time.sleep(1.5)    \n",
        "        \n",
        "    # build the google query\n",
        "    search_url = \"http://www.google.com/search?q={q}&tbm=isch\"\n",
        "\n",
        "    # load the page\n",
        "    wd.get(search_url.format(q=query))\n",
        "\n",
        "    image_urls = []\n",
        "    image_count = 0\n",
        "    results_start = 0\n",
        "    last_height=0\n",
        "    \n",
        "    while image_count < max_links_to_fetch:\n",
        "        \n",
        "        # get all image thumbnail results\n",
        "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
        "        number_results = len(thumbnail_results)\n",
        "        \n",
        "        for img in thumbnail_results[results_start:number_results]:\n",
        "            # try to click every thumbnail such that we can get the real image behind it\n",
        "            try:\n",
        "                img.click()\n",
        "                time.sleep(sleep_between_interactions)\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "            # extract image urls    \n",
        "            actual_images = wd.find_elements_by_css_selector('img.n3VNCb')\n",
        "            for actual_image in actual_images:\n",
        "                if actual_image.get_attribute('src') and 'http' in actual_image.get_attribute('src'):\n",
        "                    image_urls.append(actual_image.get_attribute('src'))              \n",
        "                    image_count +=1     \n",
        "                    if image_count == max_links_to_fetch:\n",
        "                        print(f\"Found: {len(image_urls)} image links, done!\")\n",
        "                        return image_urls \n",
        "         \n",
        "        scroll_to_end(wd)\n",
        "        new_height = wd.execute_script(\"return document.body.scrollHeight\")\n",
        "        \n",
        "        if new_height == last_height:\n",
        "            try:\n",
        "                    load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
        "                    time.sleep(2)\n",
        "                    wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
        "                    print(\"Loading more images..\")\n",
        "            \n",
        "            except:\n",
        "                    print(\"You arrived at the end of the page...\")\n",
        "                    return image_urls \n",
        "        else:\n",
        "       \n",
        "            last_height = new_height\n",
        "            \n",
        "\n",
        "        # move the result startpoint further down\n",
        "        results_start = len(thumbnail_results)\n",
        "\n",
        "\n",
        "    return image_urls "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBVu7VTo4mer"
      },
      "source": [
        "def save_images(links,path_folder,data_name,max_images):\n",
        "    \n",
        "    data_name=data_name.replace(\" \",\"_\")\n",
        "    file_path=path_folder+\"/\"+data_name+\"_\"\n",
        "\n",
        "    #vertical image\n",
        "    min_ratio=4/5\n",
        "    #horizontal image\n",
        "    max_ratio=16/9\n",
        "  \n",
        "    #list composed of element=[image,image_dimension]\n",
        "    saved_list=[]\n",
        "  \n",
        "    #remove duplicated links\n",
        "    links= list(dict.fromkeys(links))\n",
        "\n",
        "    for link in links:\n",
        "           try:\n",
        "                image_content = requests.get(link).content\n",
        "                image_file = io.BytesIO(image_content)\n",
        "                image = Image.open(image_file).convert('RGB')\n",
        "                width, height = image.size\n",
        "                ratio=width/height\n",
        "                if ratio>=min_ratio and ratio<=max_ratio\n",
        "                    saved_list.append([image,width*height])\n",
        "           except:\n",
        "                continue\n",
        "    \n",
        "    # descending sorting according to image dimension width*height\n",
        "    saved_list=sorted(saved_list,key=lambda saved_list:saved_list[1], reverse=True)\n",
        "    \n",
        "    pbar = tqdm(total=max_images)\n",
        "  \n",
        "    #save the first max_images° images\n",
        "    for i in range(max_images):\n",
        "            name =file_path+str(i)+\".jpeg\"\n",
        "            with open(name, 'wb') as f:\n",
        "                    saved_list[i][0].save(f, \"JPEG\", quality=95)   \n",
        "            f.close()\n",
        "            pbar.update(1)\n",
        "    pbar.close()                           "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E-t0BjB4mes"
      },
      "source": [
        "def search_and_download(search_term:str,wd_driver,target_path,number_images=50):\n",
        "    target_folder = os.path.join(target_path,'_'.join(search_term.lower().split(' ')))\n",
        "\n",
        "    if not os.path.exists(target_folder):\n",
        "        os.makedirs(target_folder)\n",
        "    \n",
        "    print(\"Getting links..\")\n",
        "    with wd_driver as wd:\n",
        "        res = fetch_image_urls(search_term, number_images*15, wd=wd, sleep_between_interactions=0.15)\n",
        "        \n",
        "    print(\"Getting images..\")\n",
        "    time.sleep(1)\n",
        "    save_images(res,target_folder,search_term,number_images*3)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86v77XZR4met"
      },
      "source": [
        "cnt=0\r\n",
        "for food in empty_folders:\r\n",
        "  cnt+=1\r\n",
        "  print(\"Collecting \"+str(cnt)+\"° class of food :\"+str(food))\r\n",
        "  WD_DRIVER = webdriver.Chrome('chromedriver',options=chrome_options)\r\n",
        "  search_and_download(food, WD_DRIVER)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}